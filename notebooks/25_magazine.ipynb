{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl, Load and Split Magazines from the Church of Jesus Christ of Latter-day Saints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models.load_utils import load_docs_from_jsonl, save_docs_to_jsonl\n",
    "from models.split_model import MarkdownSyntacticEmbeddingSplitter\n",
    "\n",
    "from models.crawl_utils import get_page, save_page\n",
    "from models.load_magazine import MagazineLoader\n",
    "from models.load_utils import save_docs_to_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "magazine_urls = [\n",
    "    # 'https://www.churchofjesuschrist.org/study/magazines/liahona?lang=eng',\n",
    "    # 'https://www.churchofjesuschrist.org/study/magazines/ya-weekly?lang=eng',\n",
    "    # 'https://www.churchofjesuschrist.org/study/magazines/for-the-strength-of-youth?lang=eng',\n",
    "    # 'https://www.churchofjesuschrist.org/study/magazines/for-the-strength-of-youth/new-era-19712020?lang=eng',\n",
    "    'https://www.churchofjesuschrist.org/study/magazines/friend?lang=eng',\n",
    "    # 'https://www.churchofjesuschrist.org/study/magazines/ensign-19712020?lang=eng',\n",
    "]\n",
    "base_dir = '../data/raw/magazines'\n",
    "bs_parser = 'html.parser'\n",
    "delay_seconds = 10\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_issue_link(url: str) -> bool:\n",
    "    path_components = urlparse(url).path.split('/')\n",
    "    # print('is_issue_link', url, path_components)\n",
    "    if len(path_components) < 5:\n",
    "        return False\n",
    "    elif path_components[4] == 'new-era-19712020':\n",
    "        # new-era issue links must have 6 path components\n",
    "        return len(path_components) == 6\n",
    "    else:\n",
    "        # all other issue links must have 5 components (first component is empty)\n",
    "        return len(path_components) == 5\n",
    "\n",
    "\n",
    "def get_issue_links(base_url, html):\n",
    "    soup = BeautifulSoup(html, bs_parser)\n",
    "    return [urljoin(base_url, a['href']) for a in soup.find_all('a', href=True) \\\n",
    "            if _is_issue_link(urljoin(base_url, a['href']))]\n",
    "\n",
    "def get_year_month_links(url, html):\n",
    "    links = get_issue_links(url, html)\n",
    "    year_month_links = []\n",
    "    for link in tqdm(links):\n",
    "        path_components = urlparse(link).path.split('/')\n",
    "        # print('link and components', link, path_components)\n",
    "        if len(path_components[-1]) == 2 or path_components[-1].endswith('-se'):\n",
    "            # year-month link\n",
    "            # print('year-month link', link)\n",
    "            year_month_links.append(link)\n",
    "        elif len(path_components[-1]) == 4:\n",
    "            # year_only_link\n",
    "            # print('year-only link', link)\n",
    "            status_code, html = get_page(link, delay_seconds)\n",
    "            if status_code != 200:\n",
    "                print(f\"Status code={status_code} url={link}\")\n",
    "                continue\n",
    "            new_links = get_issue_links(link, html)\n",
    "            for new_link in new_links:\n",
    "                # print('issue link', new_link)\n",
    "                year_month_links.append(new_link)\n",
    "        else:\n",
    "            print('unexpected link', link, path_components[-1])\n",
    "    return year_month_links\n",
    "\n",
    "def _is_article_link(url: str) -> bool:\n",
    "    path_components = urlparse(url).path.split('/')\n",
    "    # # must be 6 or 7 components (first component is empty)\n",
    "    return (len(path_components) == 6 or len(path_components) == 7) and \\\n",
    "        path_components[-2] != 'new-era-19712020' and path_components[-1] != 'contents'\n",
    "\n",
    "\n",
    "def get_article_links(base_url, html):\n",
    "    soup = BeautifulSoup(html, bs_parser)\n",
    "    return [urljoin(base_url, a['href']) for a in soup.find_all('a', href=True) \\\n",
    "            if _is_article_link(urljoin(base_url, a['href']))]\n",
    "\n",
    "\n",
    "def get_article_path(url):\n",
    "    path_components = urlparse(url).path.split('/')\n",
    "    path = '_'.join(path_components[2:])\n",
    "    return os.path.join(base_dir, f\"{path}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in tqdm(magazine_urls):\n",
    "    status_code, html = get_page(url, delay_seconds)\n",
    "    if status_code != 200:\n",
    "        print(f\"Status code={status_code} url={url}\")\n",
    "        continue\n",
    "    year_month_links = get_year_month_links(url, html)\n",
    "    print('year-month-links', url, len(year_month_links))\n",
    "    for link in tqdm(year_month_links):\n",
    "        print('year-month link', link)\n",
    "        # skip conference issues\n",
    "        path_components = urlparse(link).path.split('/')\n",
    "        if path_components[2] == 'ensign' and path_components[4] in ['05', '11']:\n",
    "            continue\n",
    "        if path_components[2] == 'liahona' and path_components[4] in ['05', '11'] and f\"{path_components[3]}{path_components[4]}\" >= '200211':\n",
    "            continue\n",
    "        # if path_components[3] > '1999':\n",
    "        #     continue\n",
    "        status_code, html = get_page(link, delay_seconds)\n",
    "        if status_code != 200:\n",
    "            print(f\"Status code={status_code} url={url}\")\n",
    "            continue\n",
    "        article_links = get_article_links(link, html)\n",
    "        for article_link in tqdm(article_links):\n",
    "            path = get_article_path(article_link)\n",
    "            # print('path', path, article_link)\n",
    "            if os.path.exists(path):\n",
    "                continue\n",
    "            print(\"    \", path)\n",
    "            status_code, html = get_page(article_link, delay_seconds)\n",
    "            if status_code != 200:\n",
    "                print(f\"Status code={status_code} url={article_link}\")\n",
    "                continue\n",
    "            save_page(path, article_link, html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "input_dir = '../data/raw/magazines'\n",
    "output_dir = '../data/load/magazines/'\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = MagazineLoader(input_dir)\n",
    "docs = loader.load(verbose=True)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].metadata)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = os.path.join(output_dir, f\"{today}.jsonl\")\n",
    "\n",
    "save_docs_to_jsonl(docs, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "input_path = '../data/load/magazines/2023-12-30.jsonl'\n",
    "output_dir = '../data/split/magazines/'\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_docs_from_jsonl(input_path)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = MarkdownSyntacticEmbeddingSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs, verbose=True)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, split in enumerate(splits[:10]):\n",
    "    print(ix, split.metadata[\"url\"], split.metadata[\"title\"])\n",
    "    print(split.page_content)\n",
    "    print(\"\\n!!! SPLIT !!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(output_dir, f\"{today}.jsonl\")\n",
    "save_docs_to_jsonl(splits, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models-5vTMPdpX-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
